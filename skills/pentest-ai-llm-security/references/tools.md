# Tools

# AI/LLM Security Tools

## Common response fields

- success: boolean
- stdout, stderr: command output
- injection_successful: boolean indicator of prompt injection success
- extracted_data: any data leaked from the LLM context

## LLM vulnerability scanning

- garak_scan(target, probes="all", generators="", detectors="", output_file="", additional_args=""): Automated LLM vulnerability scanner. probes: "prompt_injection"|"data_leak"|"encoding"|"all". Returns vulnerability report with confidence scores.
- rebuff_detect(prompt, max_heuristic_score=0.75, max_vector_score=0.9, max_model_score=0.9): Test prompt injection detection. Returns detection scores and bypass success.

## Prompt injection testing

- prompt_inject(target_url, payload, method="POST", field="prompt", headers="", additional_args=""): Send prompt injection payload to LLM endpoint. Returns model response for analysis.
- prompt_fuzzer(target_url, technique="all", iterations=50, field="prompt", additional_args=""): Automated prompt injection fuzzing. technique: "role_play"|"encoding"|"delimiter"|"instruction_override"|"all".

## Output analysis

- llm_output_analyzer(response, check_xss=True, check_sqli=True, check_cmdi=True): Analyze LLM output for injection payloads that could exploit downstream consumers.
- llm_api_intercept(target_url, proxy_port=8080, log_file=""): Proxy LLM API calls to capture full prompt/response chains for analysis.
