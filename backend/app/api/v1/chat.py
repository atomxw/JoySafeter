"""
Module: Chat API (Production Ready)

Overview:
- åŸºäº LangGraph æ„å»ºçš„ç”Ÿäº§çº§å¯¹è¯æ¥å£
- æ”¯æŒæµå¼ (SSE) å’Œéæµå¼è°ƒç”¨
- å®ç°äº†å®Œæ•´çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šå¯åŠ¨ã€åœæ­¢ã€æ–­è¿ä¿æŠ¤ã€æ•°æ®æŒä¹…åŒ–
- æ ‡å‡†åŒ–çš„å‰ç«¯é€šä¿¡åè®®

Dependencies:
- Task Manager: ç”¨äºç®¡ç†å¼‚æ­¥ä»»åŠ¡çš„å–æ¶ˆå’Œåœæ­¢
- Database: å¼‚æ­¥ SQLAlchemy ä¼šè¯
- LangGraph: v2 äº‹ä»¶æµå¤„ç†

Protocol (SSE):
All events follow this JSON structure in the `data` field:
{
  "type": "content" | "tool_start" | "tool_end" | "status" | "error" | "done",
  "thread_id": string,
  "run_id": string,          // ç”¨äºå‰ç«¯å…³è”æ¶ˆæ¯å—
  "node_name": string,       // å½“å‰èŠ‚ç‚¹ (e.g., "agent", "tools")
  "timestamp": number,       // æ¯«ç§’çº§æ—¶é—´æˆ³
  "tags": string[],          // æ ‡ç­¾
  "data": any                // å…·ä½“è½½è· (delta, tool_input, etc.)
}
"""

import asyncio
import uuid
from typing import Any, AsyncGenerator, Dict

from fastapi import APIRouter, Body, Depends, Request
from fastapi.responses import StreamingResponse
from langchain.messages import AIMessage, HumanMessage
from langchain_core.messages.base import BaseMessage
from langchain_core.runnables import RunnableConfig
from loguru import logger
from pydantic import BaseModel
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.common.dependencies import CurrentUser
from app.common.exceptions import (
    raise_client_closed_error,
    raise_internal_error,
    raise_not_found_error,
)
from app.core.agent.checkpointer.checkpointer import get_checkpointer
from app.core.agent.sample_agent import get_agent
from app.core.database import AsyncSessionLocal, get_db
from app.core.settings import settings
from app.models import Conversation, Message
from app.schemas import BaseResponse, ChatRequest, ChatResponse
from app.services.graph_service import GraphService
from app.utils.datetime import utc_now
from app.utils.stream_event_handler import StreamEventHandler, StreamState
from app.utils.task_manager import task_manager

# Note: graph_cache is no longer used - we use Checkpointer for state persistence

router = APIRouter(prefix="/v1/chat", tags=["Chat"])


# ==================== Data Models & Helpers ====================


class StopRequest(BaseModel):
    thread_id: str = Body(..., description="Conversation thread ID")


class ResumeRequest(BaseModel):
    thread_id: str = Body(..., description="Conversation thread ID")
    command: Dict[str, Any] = Body(..., description="Command object with update and/or goto")


def _bind_log(request: Request, **kwargs):
    """ç»‘å®šä¸Šä¸‹æ–‡æ—¥å¿—"""
    trace_id = getattr(request.state, "trace_id", "-")
    return logger.bind(trace_id=trace_id, **kwargs)


async def safe_get_state(
    graph: Any, config: RunnableConfig, max_retries: int = 3, initial_delay: float = 0.1, log: Any = None
) -> Any:
    """
    å®‰å…¨åœ°è·å–å›¾çŠ¶æ€ï¼Œå¸¦é‡è¯•æœºåˆ¶ä»¥é¿å…è¿æ¥å†²çªã€‚

    Args:
        graph: LangGraph å›¾å®ä¾‹
        config: RunnableConfig é…ç½®
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        initial_delay: åˆå§‹å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œæ¯æ¬¡é‡è¯•ä¼šç¿»å€
        log: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰

    Returns:
        å›¾çŠ¶æ€å¿«ç…§

    Raises:
        Exception: å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    """
    if log is None:
        log = logger

    last_error = None
    delay = initial_delay

    for attempt in range(max_retries):
        try:
            snap = await graph.aget_state(config)
            return snap
        except Exception as e:
            last_error = e
            error_msg = str(e)

            # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥å†²çªé”™è¯¯
            is_connection_error = (
                "another command is already in progress" in error_msg.lower() or "connection" in error_msg.lower()
            )

            # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œä¸å†é‡è¯•
            if attempt >= max_retries - 1:
                break

            # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
            if is_connection_error:
                log.debug(
                    f"Connection conflict detected (attempt {attempt + 1}/{max_retries}), "
                    f"retrying after {delay:.2f}s delay"
                )
                await asyncio.sleep(delay)
                delay *= 2  # æŒ‡æ•°é€€é¿
            else:
                # å¦‚æœä¸æ˜¯è¿æ¥é”™è¯¯ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­é‡è¯•ï¼ˆå¯èƒ½åªæ˜¯ä¸´æ—¶é—®é¢˜ï¼‰
                log.warning(f"Failed to get state (attempt {attempt + 1}/{max_retries}): {e}")
                await asyncio.sleep(delay)
                delay *= 2

    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    log.error(f"Failed to get state after {max_retries} attempts: {last_error}")
    if last_error is not None:
        raise last_error
    raise RuntimeError("Failed to get state after all retries")


# ==================== Persistence Logic ====================


async def save_run_result(thread_id: str, state: StreamState, log) -> None:
    """
    ä¿å­˜è¿è¡Œç»“æœçš„é€šç”¨é€»è¾‘ã€‚
    å³ä½¿æ˜¯åœ¨ finally å—ä¸­è°ƒç”¨ï¼Œä¹Ÿä½¿ç”¨æ–°çš„ DB Session ç¡®ä¿è¿æ¥å¯ç”¨ã€‚
    """
    if not state.assistant_content and not state.all_messages:
        return

    # å¦‚æœæ²¡æœ‰ä» Graph æ‹¿åˆ°å®Œæ•´çš„ messages å¯¹è±¡ï¼ˆä¾‹å¦‚è¢«ä¸­æ–­ï¼‰ï¼Œ
    # åˆ™ä½¿ç”¨ç´¯ç§¯çš„æ–‡æœ¬æ„å»ºå…œåº•æ¶ˆæ¯
    if not state.all_messages and state.assistant_content:
        log.warning(f"Using fallback content accumulation for thread {thread_id}")
        state.all_messages = [AIMessage(content=state.assistant_content)]

    if not state.all_messages:
        return

    try:
        async with AsyncSessionLocal() as session:
            # å¤ç”¨å·²æœ‰çš„ä¿å­˜é€»è¾‘ (ä»£ç åœ¨ä¸‹æ–¹å®šä¹‰)
            await save_assistant_message(thread_id, state.all_messages, session, update_conversation=True)
            log.info(f"Persisted messages for thread {thread_id}")
    except Exception as e:
        log.error(f"Failed to persist messages for thread {thread_id}: {e}")


# ==================== Database Operations ====================


async def get_or_create_conversation(
    thread_id: str | None,
    message: str,
    user_id: str,
    metadata: dict | None,
    db: AsyncSession,
) -> tuple[str, Conversation]:
    if not thread_id:
        # No thread_id provided, create new conversation
        thread_id = str(uuid.uuid4())
        conversation = Conversation(
            thread_id=thread_id,
            user_id=user_id,
            title=message[:50] if len(message) > 50 else message,
            meta_data=metadata or {},
        )
        db.add(conversation)
        await db.commit()
        return thread_id, conversation
    else:
        # Thread_id provided, try to find existing conversation
        result = await db.execute(
            select(Conversation).where(Conversation.thread_id == thread_id, Conversation.user_id == user_id)
        )
        conv = result.scalar_one_or_none()
        if not conv:
            # Conversation not found - create new one with the provided thread_id
            # This allows frontend to generate thread_id and let backend create conversation on first message
            conversation = Conversation(
                thread_id=thread_id,
                user_id=user_id,
                title=message[:50] if len(message) > 50 else message,
                meta_data=metadata or {},
            )
            db.add(conversation)
            await db.commit()
            await db.refresh(conversation)
            return thread_id, conversation
        return thread_id, conv


async def get_user_config(user_id: str, thread_id: str, db: AsyncSession):
    """è·å–ç”¨æˆ·é…ç½®å’Œ LLM å‚æ•°"""
    from loguru import logger

    from app.common.exceptions import NotFoundException
    from app.core.agent.langfuse_callback import get_langfuse_callbacks
    from app.core.model.utils.credential_resolver import LLMCredentialResolver

    get_langfuse_callbacks(enabled=settings.langfuse_enabled)

    config: RunnableConfig = {
        "configurable": {"thread_id": thread_id, "user_id": str(user_id)},
        "recursion_limit": 100,
        "callbacks": get_langfuse_callbacks(enabled=settings.langfuse_enabled),
    }

    # ä½¿ç”¨ç»Ÿä¸€çš„ LLMCredentialResolver è·å–å‡­æ®
    try:
        llm_params = await LLMCredentialResolver.get_llm_params(
            db=db,
            api_key=None,
            base_url=None,
            llm_model=None,
            max_tokens=4096,
        )

        # éªŒè¯æ˜¯å¦è·å–åˆ°æœ‰æ•ˆçš„å‡­æ®
        if not llm_params.get("api_key") or not llm_params.get("llm_model"):
            raise NotFoundException("æœªæ‰¾åˆ°é»˜è®¤æ¨¡å‹é…ç½®ï¼Œè¯·åœ¨å‰ç«¯é…ç½®æ¨¡å‹")
    except NotFoundException:
        raise
    except Exception as e:
        logger.error(f"[get_user_config] Failed to get default model from database: {e}")
        raise NotFoundException(f"è·å–é»˜è®¤æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")

    return config, {}, llm_params


async def save_user_message(thread_id: str, message: str, metadata: dict | None, db: AsyncSession):
    user_message = Message(
        thread_id=thread_id,
        role="user",
        content=message,
        meta_data=metadata or {},
    )
    db.add(user_message)
    await db.commit()


async def save_assistant_message(
    thread_id: str, messages: list[BaseMessage], db: AsyncSession, update_conversation: bool = True
):
    """ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯ï¼Œæ”¯æŒæå– Tool Calls"""
    # æ‰¾åˆ°æœ€åä¸€æ¡ AI æ¶ˆæ¯
    ai_msg = next((m for m in reversed(messages) if isinstance(m, AIMessage)), None)
    if not ai_msg:
        return

    meta_data = dict(ai_msg.additional_kwargs) if ai_msg.additional_kwargs else {}

    # æå– Tool Calls (ç®€åŒ–é€»è¾‘)
    if hasattr(ai_msg, "tool_calls") and ai_msg.tool_calls:
        tool_calls_data = []
        for tc in ai_msg.tool_calls:
            # å°è¯•æ‰¾åˆ°å¯¹åº”çš„ ToolOutput
            # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä¸¥è°¨å®ç°åº”éå†åç»­çš„ ToolMessage åŒ¹é… ID
            tool_calls_data.append({"name": tc.get("name"), "arguments": tc.get("args"), "id": tc.get("id")})
        meta_data["tool_calls"] = tool_calls_data

    message = Message(
        thread_id=thread_id,
        role="assistant",
        content=str(ai_msg.content) if ai_msg.content else "",
        meta_data=meta_data,
    )
    db.add(message)

    if update_conversation:
        result = await db.execute(select(Conversation).where(Conversation.thread_id == thread_id))
        if conv := result.scalar_one_or_none():
            conv.updated_at = utc_now()
    await db.commit()


# ==================== Event Handlers ====================
# äº‹ä»¶å¤„ç†é€»è¾‘å·²æŠ½å–åˆ° app.utils.stream_event_handler.StreamEventHandler


# ==================== Endpoints ====================


@router.post("/stop", response_model=BaseResponse[dict])
async def stop_chat(
    request: Request,
    payload: StopRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
):
    """åœæ­¢ä»»åŠ¡"""
    thread_id = payload.thread_id
    log = _bind_log(request, user_id=str(current_user.id), thread_id=thread_id)

    # éªŒè¯æƒé™
    res = await db.execute(
        select(Conversation).where(Conversation.thread_id == thread_id, Conversation.user_id == current_user.id)
    )
    if not res.scalar_one_or_none():
        # å³ä½¿æ‰¾ä¸åˆ°å¯¹è¯ï¼Œåªè¦ä»»åŠ¡å­˜åœ¨ä¹Ÿåº”è¯¥åœæ­¢
        log.warning("Stop request for unknown conversation")

    stopped = await task_manager.stop_task(thread_id)
    cancelled = False
    if stopped:
        cancelled = await task_manager.cancel_task(thread_id)

    status = "stopped" if stopped else "not_running"
    return BaseResponse(
        success=True, code=200, msg="Task status retrieved", data={"status": status, "cancelled": cancelled}
    )


@router.post("", response_model=BaseResponse[ChatResponse])
async def chat(
    request: Request,
    payload: ChatRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
):
    """éæµå¼å¯¹è¯ (ç®€åŒ–ç‰ˆï¼Œå¤ç”¨é€»è¾‘)

    æ³¨æ„: å¦‚æœå›¾é…ç½®äº†ä¸­æ–­ç‚¹ï¼Œgraph.ainvoke() ä¼šé˜»å¡ç­‰å¾… Commandã€‚
    å»ºè®®éœ€è¦ä¸­æ–­åŠŸèƒ½çš„åœºæ™¯ä½¿ç”¨æµå¼ç«¯ç‚¹ (/v1/chat/stream)ã€‚
    """
    thread_id, _ = await get_or_create_conversation(
        payload.thread_id, payload.message, current_user.id, payload.metadata, db
    )
    log = _bind_log(request, user_id=str(current_user.id), thread_id=thread_id)
    config, base_context, llm_params = await get_user_config(current_user.id, thread_id, db)

    try:
        await save_user_message(thread_id, payload.message, payload.metadata, db)

        # Get initial context from graph.variables.context if graph_id is provided
        initial_context = base_context.copy()
        if payload.graph_id:
            from app.repositories.graph import GraphRepository

            graph_repo = GraphRepository(db)
            graph_model = await graph_repo.get(payload.graph_id)
            if graph_model and graph_model.variables:
                context_vars = graph_model.variables.get("context", {})
                if context_vars:
                    # Convert ContextVariable objects to simple values
                    for key, value in context_vars.items():
                        if isinstance(value, dict) and "value" in value:
                            initial_context[key] = value["value"]
                        else:
                            initial_context[key] = value

        # Create graph: use default agent if graph_id is None, otherwise use graph from database
        if payload.graph_id is None:
            log.info("[Chat API] Using default agent (graph_id is None)")
            graph = await get_agent(
                checkpointer=get_checkpointer(),
                llm_model=llm_params["llm_model"],
                api_key=llm_params["api_key"],
                base_url=llm_params["base_url"],
                max_tokens=llm_params["max_tokens"],
                user_id=str(current_user.id),
            )
        else:
            graph_service = GraphService(db)
            graph = await graph_service.create_graph_by_graph_id(
                graph_id=payload.graph_id,
                llm_model=llm_params["llm_model"],
                api_key=llm_params["api_key"],
                base_url=llm_params["base_url"],
                max_tokens=llm_params["max_tokens"],
                user_id=current_user.id,
                current_user=current_user,
            )

        # ä» metadata ä¸­æå–æ–‡ä»¶ä¿¡æ¯å¹¶æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
        files = payload.metadata.get("files", [])
        if files:
            log.info(f"[Chat API] ğŸ“ å‘ç° {len(files)} ä¸ªæ–‡ä»¶: {files}")
            file_info = "\n\nAttached files:\n" + "\n".join([f"- {f['filename']}: {f['path']}" for f in files])
            enriched_message = payload.message + file_info
            log.info(f"[Chat API] âœ… æ¶ˆæ¯å·²åŒ…å«æ–‡ä»¶è·¯å¾„ï¼Œé•¿åº¦: {len(enriched_message)}")
        else:
            log.debug("[Chat API] â„¹ï¸  æ²¡æœ‰å‘ç°æ–‡ä»¶é™„ä»¶")
            enriched_message = payload.message

        # æ³¨å†Œä»»åŠ¡ä»¥æ”¯æŒéæµå¼å–æ¶ˆ
        invoke_task = asyncio.create_task(
            graph.ainvoke(
                {"messages": [HumanMessage(content=enriched_message)], "context": initial_context}, config=config
            )
        )
        await task_manager.register_task(thread_id, invoke_task)

        try:
            result = await invoke_task
        except asyncio.CancelledError:
            raise_client_closed_error("Cancelled")
        finally:
            await task_manager.unregister_task(thread_id)
            # Cleanup shared backend if exists
            if hasattr(graph, "_cleanup_backend"):
                try:
                    await graph._cleanup_backend()
                except Exception as e:
                    log.warning(f"[Chat API] Failed to cleanup backend: {e}")

        messages = result["messages"]
        await save_assistant_message(thread_id, messages, db)

        return BaseResponse(
            success=True,
            code=200,
            msg="Chat completed successfully",
            data=ChatResponse(
                thread_id=thread_id,
                response=messages[-1].content if messages else "",
                duration_ms=0,  # éœ€è‡ªè¡Œæ·»åŠ è®¡æ—¶é€»è¾‘
            ),
        )
    except Exception as e:
        log.error(f"Chat failed: {e}")
        # Ensure backend cleanup even if error occurs before finally block
        # (though finally should always execute, this is extra safety)
        if "graph" in locals() and hasattr(graph, "_cleanup_backend"):
            try:
                await graph._cleanup_backend()
            except Exception as cleanup_err:
                log.warning(f"[Chat API] Failed to cleanup backend in error handler: {cleanup_err}")
        raise_internal_error(str(e))


@router.post("/stream", response_class=StreamingResponse)
async def chat_stream(
    request: Request,
    payload: ChatRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
):
    """æµå¼å¯¹è¯ (SSE) - ç”Ÿäº§çº§å®ç°"""
    log = _bind_log(request, user_id=str(current_user.id))

    # 1. å‡†å¤‡ç¯å¢ƒ
    thread_id, _ = await get_or_create_conversation(
        payload.thread_id, payload.message, current_user.id, payload.metadata, db
    )
    await save_user_message(thread_id, payload.message, payload.metadata, db)

    config, base_context, llm_params = await get_user_config(current_user.id, thread_id, db)

    # Get initial context from graph.variables.context if graph_id is provided
    initial_context = base_context.copy()
    if payload.graph_id:
        from app.repositories.graph import GraphRepository

        graph_repo = GraphRepository(db)
        graph_model = await graph_repo.get(payload.graph_id)
        if graph_model and graph_model.variables:
            context_vars = graph_model.variables.get("context", {})
            if context_vars:
                # Convert ContextVariable objects to simple values
                for key, value in context_vars.items():
                    if isinstance(value, dict) and "value" in value:
                        initial_context[key] = value["value"]
                    else:
                        initial_context[key] = value

    # 2. æå‰æ³¨å†Œä»»åŠ¡ï¼Œç¡®ä¿åœæ­¢è¯·æ±‚èƒ½æ‰¾åˆ°ç›®æ ‡
    current_task = asyncio.current_task()
    if current_task:
        await task_manager.register_task(thread_id, current_task)

    async def event_generator() -> AsyncGenerator[str, None]:
        state = StreamState(thread_id)
        handler = StreamEventHandler()

        # å‘é€åˆå§‹çŠ¶æ€
        yield handler.format_sse("status", {"status": "connected", "_meta": {"node_name": "system"}}, thread_id)

        try:
            # 3. åˆ›å»ºå›¾: å¦‚æœ graph_id ä¸º Noneï¼Œä½¿ç”¨é»˜è®¤ Agentï¼Œå¦åˆ™ä»æ•°æ®åº“åŠ è½½å›¾
            if payload.graph_id is None:
                log.info("[Chat API Stream] Using default agent (graph_id is None)")
                graph = await get_agent(
                    checkpointer=get_checkpointer(),
                    llm_model=llm_params["llm_model"],
                    api_key=llm_params["api_key"],
                    base_url=llm_params["base_url"],
                    max_tokens=llm_params["max_tokens"],
                    user_id=str(current_user.id),
                )
            else:
                graph_service = GraphService(db)
                graph = await graph_service.create_graph_by_graph_id(
                    graph_id=payload.graph_id,
                    llm_model=llm_params["llm_model"],
                    api_key=llm_params["api_key"],
                    base_url=llm_params["base_url"],
                    max_tokens=llm_params["max_tokens"],
                    user_id=current_user.id,
                    current_user=current_user,
                )

            # 5. ä» metadata ä¸­æå–æ–‡ä»¶ä¿¡æ¯å¹¶æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
            files = payload.metadata.get("files", [])
            if files:
                log.info(f"[Chat API Stream] ğŸ“ å‘ç° {len(files)} ä¸ªæ–‡ä»¶: {files}")
                file_info = "\n\nAttached files:\n" + "\n".join([f"- {f['filename']}: {f['path']}" for f in files])
                enriched_message = payload.message + file_info
                log.info(f"[Chat API Stream] âœ… æ¶ˆæ¯å·²åŒ…å«æ–‡ä»¶è·¯å¾„ï¼Œé•¿åº¦: {len(enriched_message)}")
            else:
                log.debug("[Chat API Stream] â„¹ï¸  æ²¡æœ‰å‘ç°æ–‡ä»¶é™„ä»¶")
                enriched_message = payload.message

            # 6. äº‹ä»¶å¾ªç¯
            async for event in graph.astream_events(
                {"messages": [HumanMessage(content=enriched_message)], "context": initial_context},
                config=config,
                version="v2",
            ):
                # log.info(f"Event: {event}")
                # A. åœæ­¢æ£€æµ‹
                if await task_manager.is_stopped(thread_id):
                    state.stopped = True
                    log.info(f"Task stopped by user: {thread_id}")
                    break

                # B. äº‹ä»¶åˆ†å‘
                if isinstance(event, dict):
                    event_dict = event
                else:
                    # Convert event to dict if needed
                    event_dict = {"event": str(type(event).__name__), "data": event} if event else {}
                event_type = event_dict.get("event")
                event_name = event_dict.get("name", "")
                metadata = event_dict.get("metadata", {}) if isinstance(event_dict.get("metadata"), dict) else {}
                langgraph_node = metadata.get("langgraph_node")

                # åˆ¤æ–­æ˜¯å¦æ˜¯èŠ‚ç‚¹äº‹ä»¶ï¼ˆä¸æ˜¯å·¥å…·æˆ–LLMçš„å†…éƒ¨äº‹ä»¶ï¼‰
                is_node_event = langgraph_node is not None or (
                    event_name
                    and "node" in event_name.lower()
                    and "tool" not in event_name.lower()
                    and "model" not in event_name.lower()
                    and "llm" not in event_name.lower()
                    and "chat" not in event_name.lower()
                )

                if event_type == "on_chat_model_start":
                    yield await handler.handle_chat_model_start(event_dict, state)  # type: ignore[arg-type]

                elif event_type == "on_chat_model_stream":
                    if sse := await handler.handle_chat_model_stream(event_dict, state):  # type: ignore[arg-type]
                        yield sse

                elif event_type == "on_chat_model_end":
                    yield await handler.handle_chat_model_end(event_dict, state)  # type: ignore[arg-type]

                elif event_type == "on_tool_start":
                    yield await handler.handle_tool_start(event_dict, state)  # type: ignore[arg-type]

                elif event_type == "on_tool_end":
                    yield await handler.handle_tool_end(event_dict, state)  # type: ignore[arg-type]

                # èŠ‚ç‚¹ç”Ÿå‘½å‘¨æœŸäº‹ä»¶
                elif event_type == "on_chain_start" and is_node_event:
                    yield await handler.handle_node_start(event_dict, state)  # type: ignore[arg-type]

                elif event_type == "on_chain_end":
                    # å¦‚æœæ˜¯èŠ‚ç‚¹ç»“æŸäº‹ä»¶ï¼Œå‘é€èŠ‚ç‚¹ç»“æŸäº‹ä»¶ï¼ˆå¯èƒ½è¿”å›å¤šä¸ªäº‹ä»¶ï¼‰
                    if is_node_event:
                        result = await handler.handle_node_end(event_dict, state)  # type: ignore[arg-type]
                        # handle_node_end ç°åœ¨è¿”å›å¤šä¸ªäº‹ä»¶ï¼ˆç”¨æ¢è¡Œåˆ†éš”ï¼‰æˆ–å•ä¸ªäº‹ä»¶
                        if isinstance(result, str):
                            # å¦‚æœæ˜¯å•ä¸ªå­—ç¬¦ä¸²ï¼Œå¯èƒ½åŒ…å«å¤šä¸ªäº‹ä»¶ï¼ˆç”¨ \n\n åˆ†éš”ï¼‰
                            for event_str in result.split("\n\n"):
                                if event_str.strip():
                                    yield event_str.strip() + "\n\n"
                        elif isinstance(result, list):
                            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œé€ä¸ªå‘é€
                            for event_str in result:
                                if event_str.strip():
                                    yield event_str.strip() + "\n\n"
                        else:
                            # å•ä¸ªäº‹ä»¶å­—ç¬¦ä¸²
                            yield result

                    # C. æ”¶é›†å®Œæ•´æ¶ˆæ¯ (ä½†ä¸å‘é€ SSEï¼Œä»…ç”¨äºæœ€ç»ˆçŠ¶æ€ç¡®è®¤)
                    # LangGraph æœ‰æ—¶ä¼šåœ¨ on_chain_end çš„ output ä¸­åŒ…å«æœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨
                    # æˆ‘ä»¬å¯ä»¥å°è¯•æå–ä»¥ç¡®ä¿ all_messages æœ€å®Œæ•´
                    data_raw: Any = event_dict.get("data", {})
                    data: Dict[str, Any] = data_raw if isinstance(data_raw, dict) else {}  # type: ignore[assignment]
                    output = data.get("output") if isinstance(data, dict) else None
                    if output and isinstance(output, dict) and "messages" in output:
                        state.all_messages = output["messages"]

            # 5. æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­
            interrupted = False
            try:
                snap = await safe_get_state(graph, config, max_retries=3, initial_delay=0.1, log=log)
                # æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„ä»»åŠ¡ï¼ˆä¸­æ–­çŠ¶æ€ï¼‰
                if snap.tasks:
                    # å›¾å¤„äºä¸­æ–­çŠ¶æ€ï¼Œå‘é€ interrupt äº‹ä»¶
                    next_node = snap.tasks[0].target if snap.tasks else None
                    current_state = snap.values or {}

                    # æå–èŠ‚ç‚¹ä¿¡æ¯
                    node_info = {}
                    if next_node:
                        node_info["node_name"] = next_node
                        node_info["node_label"] = next_node.replace("_", " ").title()

                    interrupt_data = {
                        "node_name": node_info.get("node_name", "unknown"),
                        "node_label": node_info.get("node_label", "Unknown Node"),
                        "state": current_state,
                        "thread_id": thread_id,
                    }

                    log.info(f"Graph interrupted at node '{next_node}' | thread_id={thread_id}")
                    yield handler.format_sse("interrupt", interrupt_data, thread_id)
                    if payload.graph_id:
                        async with AsyncSessionLocal() as session:
                            result_query = await session.execute(
                                select(Conversation).where(Conversation.thread_id == thread_id)
                            )
                            if conv := result_query.scalar_one_or_none():
                                if not conv.meta_data:
                                    conv.meta_data = {}
                                conv.meta_data["interrupted_graph_id"] = str(payload.graph_id)
                                await session.commit()
                                log.debug(f"Stored graph_id in conversation metadata | thread_id={thread_id}")

                    state.interrupted = True
                    state.interrupt_node = next_node
                    state.interrupt_state = current_state
                    interrupted = True

            except Exception as e:
                # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼ˆå¯èƒ½æ˜¯è¿æ¥å†²çªï¼‰ï¼Œè®°å½•è­¦å‘Šä½†ä¸å½±å“æµç¨‹
                # ä¸­æ–­çŠ¶æ€ä¼šåœ¨ resume æ—¶é‡æ–°æ£€æŸ¥
                log.warning(f"Failed to check interrupt state (may be due to connection conflict): {e}")

            # 6. å¾ªç¯ç»“æŸå¤„ç†
            # å°è¯•æœ€åä¸€æ¬¡è·å–å®Œæ•´çŠ¶æ€ (é˜²æ­¢ on_chain_end æ²¡è§¦å‘æˆ–è¢«è·³è¿‡)
            if not state.all_messages and not state.stopped and not interrupted:
                try:
                    snap = await safe_get_state(graph, config, max_retries=2, initial_delay=0.05, log=log)
                    if snap.values and "messages" in snap.values:
                        state.all_messages = snap.values["messages"]
                except Exception as e:
                    log.warning(f"Failed to fetch final state: {e}")

            # 7. å‘é€ç»“æŸä¿¡å·ï¼ˆå¦‚æœæœªä¸­æ–­ï¼‰
            if interrupted:
                # ä¸­æ–­çŠ¶æ€ï¼Œä¸å‘é€ done äº‹ä»¶ï¼Œç­‰å¾…ç”¨æˆ·æ“ä½œ
                # äº‹ä»¶æµä¼šåœ¨è¿™é‡Œæš‚åœï¼Œç­‰å¾… /v1/chat/resume ç«¯ç‚¹è¢«è°ƒç”¨
                pass
            elif state.stopped:
                yield handler.format_sse(
                    "error",
                    {"message": "Stopped by user", "code": "stopped", "_meta": {"node_name": "system"}},
                    thread_id,
                )
            else:
                yield handler.format_sse("done", {"_meta": {"node_name": "system"}}, thread_id)

        except asyncio.CancelledError:
            log.warning(f"Client disconnected: {thread_id}")
            state.stopped = True  # æ ‡è®°ä¸ºåœæ­¢ä»¥ä¾¿åç»­ä¿å­˜é€»è¾‘çŸ¥é“çŠ¶æ€
            # æ— éœ€ yieldï¼Œå› ä¸ºå®¢æˆ·ç«¯å·²æ–­å¼€
        except Exception as e:
            import traceback

            log.error(f"Stream error: {e}, traceback: {traceback.format_exc()}")
            state.has_error = True
            yield handler.format_sse("error", {"message": str(e), "_meta": {"node_name": "system"}}, thread_id)
        finally:
            # 7. æ¸…ç†ä¸æŒä¹…åŒ– (å…³é”®ï¼šä½¿ç”¨ finally ç¡®ä¿å³ä½¿æŠ¥é”™/æ–­è¿ä¹Ÿæ‰§è¡Œ)
            await task_manager.unregister_task(thread_id)
            await save_run_result(thread_id, state, log)
            # Cleanup shared backend if exists
            if "graph" in locals() and hasattr(graph, "_cleanup_backend"):
                try:
                    await graph._cleanup_backend()
                except Exception as e:
                    log.warning(f"[Chat API Stream] Failed to cleanup backend: {e}")
            # å¦‚æœæ‰§è¡Œå®Œæˆï¼ˆéä¸­æ–­ï¼‰ï¼Œæ¸…ç† conversation ä¸­çš„ä¸­æ–­æ ‡è®°
            if not state.interrupted:
                async with AsyncSessionLocal() as session:
                    result_query = await session.execute(
                        select(Conversation).where(Conversation.thread_id == thread_id)
                    )
                    if conv := result_query.scalar_one_or_none():
                        if conv.meta_data and "interrupted_graph_id" in conv.meta_data:
                            del conv.meta_data["interrupted_graph_id"]
                            await session.commit()
                            log.debug(f"Cleared interrupt marker from conversation | thread_id={thread_id}")

    return StreamingResponse(event_generator(), media_type="text/event-stream")


@router.post("/resume", response_class=StreamingResponse)
async def chat_resume(
    request: Request,
    payload: ResumeRequest,
    current_user: CurrentUser,
    db: AsyncSession = Depends(get_db),
):
    """æ¢å¤ä¸­æ–­çš„å›¾æ‰§è¡Œ (SSE) - ä½¿ç”¨ Command æœºåˆ¶å’Œ Checkpointer"""
    log = _bind_log(request, user_id=str(current_user.id))
    thread_id = payload.thread_id

    # 1. ä» conversation è·å– graph_id å’Œç”¨æˆ·é…ç½®
    result = await db.execute(
        select(Conversation).where(Conversation.thread_id == thread_id, Conversation.user_id == str(current_user.id))
    )
    conversation = result.scalar_one_or_none()
    if not conversation:
        log.error(f"Conversation not found | thread_id={thread_id}")
        raise_not_found_error("Conversation not found.")

    # è·å– graph_idï¼ˆä» conversation.meta_data æˆ–ä» checkpointer çŠ¶æ€æ¨æ–­ï¼‰
    graph_id = None
    if (
        conversation
        and conversation.meta_data
        and isinstance(conversation.meta_data, dict)
        and "interrupted_graph_id" in conversation.meta_data
    ):
        import uuid as uuid_lib

        try:
            graph_id = uuid_lib.UUID(str(conversation.meta_data["interrupted_graph_id"]))
        except (ValueError, TypeError):
            log.warning(f"Invalid graph_id in conversation metadata | thread_id={thread_id}")

    # å¦‚æœæ— æ³•ä» conversation è·å–ï¼Œå°è¯•ä» checkpointer çŠ¶æ€æ¨æ–­
    if not graph_id:
        # ä» checkpointer è·å–æœ€æ–°çŠ¶æ€ï¼Œå°è¯•æ¨æ–­ graph_id
        # æ³¨æ„ï¼šè¿™éœ€è¦ graph_id å­˜å‚¨åœ¨çŠ¶æ€ä¸­ï¼Œæˆ–è€…é€šè¿‡å…¶ä»–æ–¹å¼è·å–
        log.warning(
            f"Graph ID not found in conversation metadata, attempting to infer from state | thread_id={thread_id}"
        )
        # å¯ä»¥å°è¯•ä»å…¶ä»–æ¥æºè·å– graph_idï¼Œä¾‹å¦‚ä»æœ€è¿‘çš„æ‰§è¡Œè®°å½•

    # 2. è·å–ç”¨æˆ·é…ç½®å’Œ LLM å‚æ•°
    config, base_context, llm_params = await get_user_config(current_user.id, thread_id, db)

    # 3. é‡æ–°æ„å»ºå›¾
    # Checkpointer ä¼šè‡ªåŠ¨æ¢å¤ä¹‹å‰çš„çŠ¶æ€
    if graph_id is None:
        raise_not_found_error("Graph ID not found in conversation metadata or state")

    # Type narrowing: graph_id is guaranteed to be UUID after check
    assert graph_id is not None

    try:
        graph_service = GraphService(db)
        graph = await graph_service.create_graph_by_graph_id(
            graph_id=graph_id,
            llm_model=llm_params["llm_model"],
            api_key=llm_params["api_key"],
            base_url=llm_params["base_url"],
            max_tokens=llm_params["max_tokens"],
            user_id=current_user.id,
            current_user=current_user,
        )
        log.info(f"Graph rebuilt for resume | thread_id={thread_id} | graph_id={graph_id}")
    except Exception as e:
        log.error(f"Failed to rebuild graph | thread_id={thread_id} | error={e}")
        raise_internal_error(f"Failed to rebuild graph: {str(e)}")

    # 4. éªŒè¯ checkpointer ä¸­æ˜¯å¦æœ‰ä¸­æ–­çŠ¶æ€
    try:
        snap = await safe_get_state(graph, config, max_retries=3, initial_delay=0.1, log=log)
        if not snap.tasks:
            log.warning(f"No interrupt state found in checkpointer | thread_id={thread_id}")
            raise_not_found_error("No interrupt state found. Execution may have completed or expired.")
    except Exception as e:
        log.error(f"Failed to verify interrupt state | thread_id={thread_id} | error={e}")
        raise_not_found_error("Failed to verify interrupt state. Execution may have expired.")

    log.info(f"Resuming graph execution | thread_id={thread_id} | graph_id={graph_id}")

    # 5. æ„é€  LangGraph Command å¯¹è±¡
    from langgraph.types import Command

    command_update = payload.command.get("update")
    command_goto = payload.command.get("goto")

    command = Command(
        update=command_update if command_update else {},
        goto=command_goto if command_goto else None,
    )

    log.info(f"Command constructed | thread_id={thread_id} | has_update={bool(command_update)} | goto={command_goto}")

    # 6. æ³¨å†Œä»»åŠ¡ä»¥æ”¯æŒå–æ¶ˆ
    current_task = asyncio.current_task()
    if current_task:
        await task_manager.register_task(thread_id, current_task)

    async def event_generator() -> AsyncGenerator[str, None]:
        state = StreamState(thread_id)
        handler = StreamEventHandler()

        # å‘é€æ¢å¤çŠ¶æ€
        yield handler.format_sse("status", {"status": "resumed", "_meta": {"node_name": "system"}}, thread_id)

        try:
            # 4. ä½¿ç”¨ Command ç»§ç»­æ‰§è¡Œ
            async for event in graph.astream_events(command, config=config, version="v2"):
                # A. åœæ­¢æ£€æµ‹
                if await task_manager.is_stopped(thread_id):
                    state.stopped = True
                    log.info(f"Task stopped by user: {thread_id}")
                    break

                # B. äº‹ä»¶åˆ†å‘ï¼ˆå¤ç”¨ chat_stream çš„é€»è¾‘ï¼‰
                if isinstance(event, dict):
                    event_dict = event
                else:
                    # Convert event to dict if needed
                    event_dict = {"event": str(type(event).__name__), "data": event} if event else {}
                event_type = event_dict.get("event")
                event_name = event_dict.get("name", "")
                metadata = event_dict.get("metadata", {}) if isinstance(event_dict.get("metadata"), dict) else {}
                langgraph_node = metadata.get("langgraph_node")

                is_node_event = langgraph_node is not None or (
                    event_name
                    and "node" in event_name.lower()
                    and "tool" not in event_name.lower()
                    and "model" not in event_name.lower()
                    and "llm" not in event_name.lower()
                    and "chat" not in event_name.lower()
                )

                if event_type == "on_chat_model_start":
                    yield await handler.handle_chat_model_start(event_dict, state)  # type: ignore[arg-type]

                elif event_type == "on_chat_model_stream":
                    if sse := await handler.handle_chat_model_stream(event_dict, state):  # type: ignore[arg-type]
                        yield sse

                elif event_type == "on_chat_model_end":
                    yield await handler.handle_chat_model_end(event_dict, state)  # type: ignore[arg-type]

                elif event_type == "on_tool_start":
                    yield await handler.handle_tool_start(event_dict, state)  # type: ignore[arg-type]

                elif event_type == "on_tool_end":
                    yield await handler.handle_tool_end(event_dict, state)  # type: ignore[arg-type]

                elif event_type == "on_chain_start" and is_node_event:
                    yield await handler.handle_node_start(event_dict, state)  # type: ignore[arg-type]

                elif event_type == "on_chain_end":
                    if is_node_event:
                        result = await handler.handle_node_end(event_dict, state)  # type: ignore[arg-type]
                        # handle_node_end ç°åœ¨è¿”å›å¤šä¸ªäº‹ä»¶ï¼ˆç”¨æ¢è¡Œåˆ†éš”ï¼‰æˆ–å•ä¸ªäº‹ä»¶
                        if result and isinstance(result, str):
                            # å¦‚æœåŒ…å«å¤šä¸ªäº‹ä»¶ï¼ˆç”¨ \n\n åˆ†éš”ï¼‰ï¼Œé€ä¸ªå‘é€
                            for event_str in result.split("\n\n"):
                                if event_str.strip():
                                    yield event_str.strip() + "\n\n"

                    data_raw: Any = event_dict.get("data", {})
                    data: Dict[str, Any] = data_raw if isinstance(data_raw, dict) else {}  # type: ignore[assignment]
                    output = data.get("output") if isinstance(data, dict) else None
                    if output and isinstance(output, dict) and "messages" in output:
                        state.all_messages = output["messages"]

            # 5. æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„ä¸­æ–­
            interrupted = False
            try:
                snap = await safe_get_state(graph, config, max_retries=3, initial_delay=0.1, log=log)
                if snap.tasks:
                    next_node = snap.tasks[0].target if snap.tasks else None
                    current_state = snap.values or {}

                    node_info = {}
                    if next_node:
                        node_info["node_name"] = next_node
                        node_info["node_label"] = next_node.replace("_", " ").title()

                    interrupt_data = {
                        "node_name": node_info.get("node_name", "unknown"),
                        "node_label": node_info.get("node_label", "Unknown Node"),
                        "state": current_state,
                        "thread_id": thread_id,
                    }

                    log.info(f"Graph interrupted again at node '{next_node}' | thread_id={thread_id}")
                    yield handler.format_sse("interrupt", interrupt_data, thread_id)

                    state.interrupted = True
                    state.interrupt_node = next_node
                    state.interrupt_state = current_state
                    interrupted = True

            except Exception as e:
                # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼ˆå¯èƒ½æ˜¯è¿æ¥å†²çªï¼‰ï¼Œè®°å½•è­¦å‘Šä½†ä¸å½±å“æµç¨‹
                # ä¸­æ–­çŠ¶æ€ä¼šåœ¨ resume æ—¶é‡æ–°æ£€æŸ¥
                log.warning(f"Failed to check interrupt state (may be due to connection conflict): {e}")

            # 6. è·å–æœ€ç»ˆçŠ¶æ€
            if not state.all_messages and not state.stopped and not interrupted:
                try:
                    snap = await safe_get_state(graph, config, max_retries=2, initial_delay=0.05, log=log)
                    if snap.values and "messages" in snap.values:
                        state.all_messages = snap.values["messages"]
                except Exception as e:
                    log.warning(f"Failed to fetch final state: {e}")

            # 7. å‘é€ç»“æŸä¿¡å·
            if interrupted:
                pass  # ç­‰å¾…ä¸‹ä¸€æ¬¡æ¢å¤
            elif state.stopped:
                yield handler.format_sse(
                    "error",
                    {"message": "Stopped by user", "code": "stopped", "_meta": {"node_name": "system"}},
                    thread_id,
                )
            else:
                # æ‰§è¡Œå®Œæˆï¼Œæ¸…ç† conversation ä¸­çš„ä¸­æ–­æ ‡è®°
                async with AsyncSessionLocal() as session:
                    result_query = await session.execute(
                        select(Conversation).where(Conversation.thread_id == thread_id)
                    )
                    if conv := result_query.scalar_one_or_none():
                        if conv.meta_data and "interrupted_graph_id" in conv.meta_data:
                            del conv.meta_data["interrupted_graph_id"]
                            await session.commit()
                            log.debug(f"Cleared interrupt marker from conversation | thread_id={thread_id}")
                yield handler.format_sse("done", {"_meta": {"node_name": "system"}}, thread_id)

        except asyncio.CancelledError:
            log.warning(f"Client disconnected: {thread_id}")
            state.stopped = True
        except Exception as e:
            log.error(f"Resume stream error: {e}")
            state.has_error = True
            yield handler.format_sse("error", {"message": str(e), "_meta": {"node_name": "system"}}, thread_id)
        finally:
            await task_manager.unregister_task(thread_id)
            await save_run_result(thread_id, state, log)
            # Cleanup shared backend if exists
            if "graph" in locals() and hasattr(graph, "_cleanup_backend"):
                try:
                    await graph._cleanup_backend()
                except Exception as e:
                    log.warning(f"[Chat API Resume] Failed to cleanup backend: {e}")
            # å¦‚æœæ‰§è¡Œå®Œæˆï¼ˆéä¸­æ–­ï¼‰ï¼Œæ¸…ç† conversation ä¸­çš„ä¸­æ–­æ ‡è®°
            if not state.interrupted:
                async with AsyncSessionLocal() as session:
                    result_query = await session.execute(
                        select(Conversation).where(Conversation.thread_id == thread_id)
                    )
                    if conv := result_query.scalar_one_or_none():
                        if conv.meta_data and "interrupted_graph_id" in conv.meta_data:
                            del conv.meta_data["interrupted_graph_id"]
                            await session.commit()
                            log.debug(f"Cleared interrupt marker from conversation | thread_id={thread_id}")

    return StreamingResponse(event_generator(), media_type="text/event-stream")
