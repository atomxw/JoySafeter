# MCP 安全检测 Benchmark 数据集介绍

## 概述

本 benchmark 数据集旨在为 MCP (Model Context Protocol) 安全检测工具提供全面、系统的评估标准。数据集采用双轨制构建策略，结合合成漏洞与真实漏洞，全面覆盖 MCP 生态系统中可能存在的各类安全威胁，为安全检测工具的能力评估提供可靠基准。通过精心设计的测试用例和详细的漏洞标注，本数据集不仅能够评估检测工具的准确率和召回率，还能够深入分析工具在不同类型漏洞、不同复杂场景下的检测能力，为安全检测工具的持续改进提供科学依据。

## 数据集构成

### 第一种：基于正常代码仓库的合成漏洞数据集 (Bench)

该数据集从正常的 MCP 代码仓库出发，基于已公开披露的 MCP 相关漏洞模式，使用大语言模型（LLM）在正常代码中注入跨文件、跨函数的复杂漏洞。数据集共包含 81 个精心设计的测试用例，每个测试用例对应一个真实的开源 MCP 服务器项目，涵盖多种应用场景和编程语言（主要包括 Python、TypeScript/JavaScript，以及少量 Rust、Go 等）。在 `repos/` 目录中，我们收集了 81 个来自 GitHub 的真实开源 MCP 服务器项目，这些项目代表了 MCP 生态系统中的各类应用场景，从开发工具集成到云服务集成，从数据库操作到 API 集成，从工作流自动化到文档处理，从安全工具到游戏引擎，几乎涵盖了 MCP 生态系统的所有主要应用领域。在 `answers/` 目录中，我们为每个仓库提供了详细的漏洞说明文件（JSON 格式），每个文件不仅记录了漏洞的标题和唯一标识符（采用 SAFE-T 编号系统），还包含了漏洞的详细技术描述、漏洞位置信息（精确到文件名、起始行号和结束行号）、漏洞实现机制和攻击向量说明。每个项目平均包含 1-3 个不同类型的漏洞，这些漏洞经过精心设计，旨在模拟真实攻击场景并考验检测工具的能力。

该数据集涵盖了 MCP 生态系统中常见的安全威胁类型，包括命令注入攻击（SAFE-T1101, SAFE-T1204），通过未正确验证的用户输入执行任意系统命令；工具投毒攻击（SAFE-T1001），在工具描述和 Schema 中嵌入恶意指令，利用 Unicode 混淆、双向文本覆盖等技术绕过检测；权限提升漏洞（SAFE-T1104），通过工具间协调链实现权限横向移动和提升；路径遍历漏洞（SAFE-T1105），利用弱验证机制读取任意文件；凭证窃取（SAFE-T1503），通过环境变量、配置文件等多阶段数据流窃取敏感凭证；数据外泄（SAFE-T1301），通过隐蔽信道、DNS 查询、外部 API 等方式泄露敏感数据；输入验证绕过（SAFE-T1102），利用 Unicode 混淆字符、编码转换等技术绕过输入验证；上下文污染（SAFE-T1801, SAFE-T1803），通过文档投毒、提示注入等方式污染 AI 模型上下文；工作流注入（SAFE-T1206），在工作流系统中注入恶意执行逻辑；配置劫持（SAFE-T1913），通过配置管理工具劫持系统配置等。这些漏洞类型不仅覆盖了传统 Web 应用安全威胁，还特别关注了 MCP 生态系统和 AI 应用场景中的新型安全威胁，为全面评估安全检测工具提供了丰富的测试场景。

为确保漏洞的真实性和检测难度，合成漏洞采用了多种高级隐藏技术。首先，我们采用了条件激活机制，通过环境变量、时间延迟、特定触发条件等方式，使恶意代码仅在特定场景下激活，这样既模拟了真实攻击中的隐蔽性，又增加了检测的难度。其次，我们采用了跨文件协作技术，将攻击逻辑分散在多个模块和文件中，通过数据流传递恶意载荷，降低单点可疑度，这使得检测工具需要具备跨文件、跨模块的深度分析能力。第三，我们使用了多种代码混淆技术，包括 Base64 编码、十六进制拼接、动态字符串构建、Unicode 混淆等，这些技术可以有效绕过简单的静态分析工具。第四，我们引入了"验证剧场"的概念，在代码中添加看似合理的安全检查和验证逻辑，实际上这些检查存在绕过方法或形同虚设，这种技术可以欺骗代码审查人员和简单的自动化工具。第五，我们将恶意功能伪装成性能监控、日志聚合、调试工具、缓存优化等正常生产功能，降低审查时的警觉性。最后，我们通过多层数据流转，使漏洞检测需要深度代码分析能力，通过多个函数调用和模块间数据传递，将恶意逻辑隐藏在看似正常的代码流程中。这些技术综合运用，使得漏洞在代码审查、SAST 扫描、运行时行为分析中都难以被发现，从而真实地检验安全工具的深度分析能力。

数据集涵盖了 MCP 生态系统的各类应用场景，包括开发工具集成（代码编辑器、调试器、反编译工具如 Ghidra、IDA、JADX、LLDB、GDB 等）、云服务集成（AWS、Azure、Vercel、Datadog、CloudWatch 等云平台服务）、数据库操作（MongoDB、Redis、SQLite、Neo4j 等数据库接口）、API 集成（GitHub、OpenAI、Wolfram Alpha、Postman 等第三方 API）、工作流自动化（n8n、Flowise 等工作流平台）、文档处理（Excel、Markdown、JSON 等文档格式处理）、安全工具（BloodHound、Mythic C2 等安全分析工具）、游戏引擎（Unity、Godot 等游戏开发工具）等 30 多种不同的应用场景和集成类型。这种广泛的应用场景覆盖确保了数据集能够代表 MCP 生态系统中的实际使用情况，为安全检测工具提供了真实世界的测试环境。

### 第二种：基于真实漏洞仓库的数据集 (GHSA)

该数据集收集了 18 个在 GitHub 上已公开披露并通过 GitHub Security Advisory (GHSA) 系统正式发布的真实漏洞仓库，所有漏洞均经过人工审核确认，确保其真实性和准确性。这些漏洞来自生产环境中的实际项目，反映了 MCP 生态系统中真实存在的安全威胁。在 `repos/` 目录中，我们保存了漏洞修复前的代码快照，这些代码快照精确对应到漏洞修复的提交（commit SHA），使得研究人员可以对比修复前后的代码，深入理解漏洞的成因和修复方法。在 `answers/` 目录中，我们为每个漏洞提供了详细的分析文件，记录了漏洞的 CWE 分类（如 CWE-77 命令注入）、GHSA 编号、详细技术描述和影响分析、漏洞位置和触发条件，以及漏洞修复提交信息。每个漏洞都有对应的 GHSA 编号和 CWE 分类，这使得数据集与业界标准对齐，便于与其他安全研究工作进行对比和验证。

该数据集包含的真实漏洞类型主要包括命令注入（CWE-77），在 `adb-mcp`、`create-mcp-server-stdio` 等项目中发现的命令注入漏洞；输入验证缺陷，多个项目中的输入验证不充分问题；权限控制问题，访问控制和权限管理相关的安全缺陷；数据泄露风险，敏感信息泄露和不当的数据处理等。这些真实漏洞的价值在于，它们都来自生产环境，反映了实际的安全威胁，涵盖了不同项目、不同场景下的真实安全问题。通过对比修复前后的代码，研究人员可以验证检测工具的能力，了解工具在真实场景中的表现。同时，使用 CWE 和 GHSA 标准分类，使得数据集便于与业界标准对齐，为安全研究提供了标准化的基础。

## 数据集统计信息

本 benchmark 数据集总共包含 99 个测试用例，其中 81 个来自合成漏洞数据集，18 个来自真实漏洞数据集。这些测试用例对应 99 个 MCP 服务器项目，涵盖了约 150+ 个独立漏洞（每个项目平均包含 1-3 个漏洞）。从代码语言分布来看，数据集主要涵盖 TypeScript/JavaScript 和 Python，这是 MCP 生态系统中最主流的编程语言，同时也包含少量 Rust、Go 等其他语言的项目，确保了数据集的语言多样性。从应用场景覆盖来看，数据集涵盖了 30+ 种不同的应用场景和集成类型，从开发工具到云服务，从数据库到 API，从工作流到文档处理，几乎涵盖了 MCP 生态系统的所有主要应用领域。

从漏洞严重程度分布来看，高危漏洞约占总数的 40%，包括命令注入、权限提升、数据外泄等可能造成严重安全后果的漏洞类型；中危漏洞约占总数的 45%，包括输入验证绕过、配置问题等可能被利用但影响相对较小的漏洞；低危漏洞约占总数的 15%，包括信息泄露、弱验证等风险相对较低的漏洞。这种分布反映了真实世界中漏洞的严重程度分布，使得数据集能够全面评估安全检测工具在不同严重程度漏洞上的检测能力。

## 数据集用途

本 benchmark 数据集的主要用途包括安全检测工具评估、检测能力基准测试、工具开发与优化，以及安全研究等多个方面。在安全检测工具评估方面，数据集可以用于评估检测工具的准确率（Precision），通过对比检测结果与标准答案，计算检测工具报告的问题中真实存在的比例；评估召回率（Recall），评估工具发现真实漏洞的能力，计算实际存在的问题中被检测出的比例；分析误报率，统计工具误报的情况，评估工具的实用性；进行漏报分析，识别工具未能检测到的漏洞类型，指导工具改进方向。这些指标的综合评估可以全面了解检测工具的性能表现。

在检测能力基准测试方面，数据集可以用于评估工具对不同类型漏洞的检测能力，了解工具在命令注入、工具投毒、权限提升等不同类型漏洞上的表现差异；评估工具对跨文件、跨函数复杂漏洞的分析能力，测试工具是否具备深度代码分析能力；评估工具对代码混淆和隐藏技术的识别能力，了解工具在面对高级隐藏技术时的表现；通过真实漏洞数据集评估工具在实际场景中的表现，验证工具在真实世界中的有效性。这些测试可以帮助研究人员全面了解检测工具的能力边界和局限性。

在工具开发与优化方面，数据集可以指导检测算法的改进，通过分析漏报和误报，找出算法的薄弱环节，指导算法的改进方向；完善检测规则和模式库，基于漏洞模式，识别新的攻击模式，完善检测规则库；评估检测工具在不同规模代码库中的性能表现，了解工具的可扩展性和性能瓶颈。在安全研究方面，数据集可以用于分析 MCP 生态系统中常见的安全威胁模式，识别漏洞的共同特征和模式；研究针对 MCP 系统的攻击技术和绕过方法，了解攻击者的技术手段；基于漏洞分析，研究有效的防御策略，为 MCP 生态系统的安全防护提供理论支持。

## 数据集优势

本 benchmark 数据集具有全面性、真实性、可扩展性和标准化等显著优势。在全面性方面，数据集覆盖了 MCP 生态系统中的主要应用场景和漏洞类型，结合合成漏洞和真实漏洞，兼顾了全面性和真实性，同时包含不同复杂度和隐蔽程度的漏洞，满足不同层次的评估需求。在真实性方面，真实漏洞数据集来自生产环境，反映了实际的安全威胁，合成漏洞采用真实代码仓库，漏洞植入技术模拟真实攻击场景，漏洞标注详细准确，包含完整的技术细节和位置信息。在可扩展性方面，数据集结构清晰，便于添加新的测试用例，支持不同编程语言和框架的扩展，可以持续更新，纳入新发现的漏洞类型。在标准化方面，数据集使用标准化的漏洞分类系统（SAFE-T、CWE、GHSA），采用统一的标注格式，便于自动化评估，包含完整的元数据信息，支持多维度的分析。

## 使用建议

在使用本 benchmark 数据集进行评估之前，建议评估人员首先熟悉数据集的目录结构和标注格式，理解漏洞分类系统（SAFE-T、CWE、GHSA），了解不同漏洞类型的特征和检测要点。在评估过程中，建议采用分阶段评估策略，先使用真实漏洞数据集进行初步评估，了解工具在真实场景中的基本表现，再使用合成漏洞数据集进行深入测试，评估工具在面对复杂隐藏技术时的检测能力。在结果分析时，不仅要关注整体指标（如准确率、召回率、F1 分数），还要深入分析不同类型漏洞的检测表现，识别工具的薄弱环节，为工具改进提供具体方向。同时，建议根据评估结果持续改进检测工具，并定期使用数据集进行回归测试，确保工具改进的有效性。

## 注意事项

在使用本 benchmark 数据集时，需要注意以下几个重要事项。首先，数据集中的代码包含故意植入的安全漏洞，这些代码仅供测试使用，严禁在生产环境中部署，任何在生产环境中使用这些代码的行为都可能造成严重的安全风险。其次，真实漏洞数据集涉及实际项目的安全问题，使用时需注意保密，避免泄露相关项目的安全信息。第三，数据集会根据新发现的漏洞类型和模式持续更新，建议定期同步最新版本，确保评估结果的时效性和准确性。最后，建议在隔离的测试环境中使用数据集，避免对生产环境造成影响，同时确保测试环境的安全性和可控性。

---

本 benchmark 数据集旨在推动 MCP 安全检测技术的发展，为构建更安全的 AI 应用生态系统贡献力量。通过提供全面、真实、标准化的评估基准，我们希望帮助安全研究人员和工具开发者更好地理解和改进 MCP 安全检测技术，最终为整个 AI 应用生态系统的安全防护做出贡献。
